#####################################################
# HW 3 Question 8 
#####################################################

# Simple linear regression
Auto <- data.frame(Auto)
lm.fit1 <- lm(formula = mpg ~ horsepower, data = Auto)
summary(lm.fit1)

# i. Yes, there is. Since both the t-test and f-test generated extremely small p values,
#    we can reject the null hypothesis and infer there is a statistically significant relationship
# ii. If we look at Adj R squared, we see that a linear model of horsepower and mpg
#     explains 60.5% of the variance in the data
# iii. Negative - more horsepower means less mpg
# iv. Using code below - predicted mpg is 24.47 
predict(lm.fit1, data.frame(horsepower = c(98)), interval = "confidence")
predict(lm.fit1, data.frame(horsepower = c(98)), interval = "prediction")

# Visualize data
attach(Auto)
plot(x = horsepower, y = mpg)
abline(lm.fit1, lwd = 3, col = "red")

# Diagnostic plots
# Based on the residuals plots, there is some evidence of non-linearity.
par(mfrow = c(2,2))
plot(lm.fit1)

#####################################################
# HW 3 Question 9 
#####################################################

# Produce matrix of scatterplots
pairs(Auto)

# Compute matrix of correlations
cor(subset(Auto, select = (-name)))

# Multivariate linear regression minus names (qualitative var)
lm.fit2 <- lm(formula = mpg ~ . - name, data = Auto)
summary(lm.fit2)

# i. Yes, overall, there is a relationship between the predictors and response
#    given that the F statistic returns a small p-value, so we can reject
#    the null hypothesis that there is no relationship 
# ii. The most statistically significant predictors are weight, year, and origin
#     according to the p-values generated by T tests
# iii. The coefficient for year suggests that that more modern cars have better mpg 
#      (.75 mpg per year!)

# Diagnostic plots
# Based of the residual plot, there is non-linearity in error
# Additionally, there is high leverage in the residuals, which skews the least squares line
par(mfrow = c(2,2))
plot(lm.fit2)

# Outliers exist as well as some studentized residuals > 3 
plot(predict(lm.fit2), rstudent(lm(lm.fit2)))

# Fit model with interaction effects; testing interaction b/w most correlated predictors
lm.fit3 <- lm(formula = mpg ~ cylinders*displacement + displacement*weight)
summary(lm.fit3)

# Transform data
# Data skewed so apply log
lm.fit4 <- lm(formula = log(mpg) ~ . - name, data = Auto)
summary(lm.fit4)
par(mfrow = c(2,2))
plot(lm.fit4)
plot(predict(lm.fit4), rstudent(lm.fit4))

#####################################################
# HW 3 Question 10
#####################################################

Carseats <- data.frame(Carseats)
lm.fit5 <- lm(formula = Sales ~ Price + Urban + US, data = Carseats)
summary(lm.fit5)
# According to the coefficients, sales decrease as price increases. If someone is from
# the city, sales too decreases. There isn't enough statistical evidence that being
# from an urban location has anything to do with sales. Additionally, people from the US
# buy more cars.

# Sales = 13.04 - 0.05(Price) - 0.02(UrbanYes) + 1.2(USYes)
# We can reject the null hypothesis for Price and USYes, based on p values of t-statistic
# and f-statistic

lm.fit6 <- lm(formula = Sales ~ Price + US, data = Carseats)
summary(lm.fit6)

# Both the models fit the data nearly equally as well, although lm.fit6 is slightly better
# Confidence intervals
confint(lm.fit6) 

# Evidence of outliers/high leverage?
# No, there are no outliers - all studentized residuals are within bound (-3, 3)
# However, there is high leverage (p+1/n = 0.0075, which a lot of points exceed)
par(mfrow = c(2,2))
plot(lm.fit6)

#####################################################
# HW 3 Question 11
#####################################################

# Creating simple linear regression
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)

# Create linear regression w/o intercept
lm.fit7 <- lm(formula = y ~ x + 0)
summary(lm.fit7)

lm.fit8 <- lm(formula = y ~ x)
summary(lm.fit8)

# Confirm in R that t-statistic formula can be written differently
(sqrt(length(x)-1) * sum(x*y)) / (sqrt(sum(x*x) * sum(y*y) - (sum(x*y))^2))

# If you swap y for x in the formula above, you get the same answer (18.73)
(sqrt(length(y)-1) * sum(y*x)) / (sqrt(sum(y*y) * sum(x*x) - (sum(y*x))^2))

# T statistic is same for both linear models with intercepts
lm.fit9 <- lm(y~x)
lm.fit10 <- lm(x~y)
summary(lm.fit9)
summary(lm.fit10)

#####################################################
# HW 3 Question 12
#####################################################

# The coefficient estimate is the same for X onto Y as Y onto X when the sum of squares 
# of the y values equals the sum of squares for the X

set.seed(1)
a <- rnorm(100)
b <- 2 * a

# Make the coefficients different
lm.fit11 <- lm(formula = b ~ a + 0)
lm.fit12 <- lm(formula = a ~ b + 0)
summary(lm.fit11)
summary(lm.fit12)

# Make the coefficients the same
c <- rnorm(100)
d <- -sample(c, 100)

lm.fit13 <- lm(formula = c ~ d + 0)
lm.fit14 <- lm(formula = d ~ c + 0)
summary(lm.fit13)
summary(lm.fit14)

#####################################################
# HW 3 Question 13
#####################################################

set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, mean = 0, sd = sqrt(.25))

y <- -1 + 0.5*x + eps
plot(x,y)
# The plot shows a positive relationship between x and y

lm.fit15 <- lm(formula = y ~ x)
abline(-1, 0.5, lwd = 3, col = "blue")
abline(lm.fit15, lwd = 3, col = "red")
summary(lm.fit15)
# The estimated coefficients were very close the actual coefficients
# The t-values show that the coefficients are statistically significant
# And the F statistic shows that we can reject the null hypothesis that 
# the coefficients are jointly significant 

# Creating a legend
legend(-2, legend = c("population fit", "model fit"), col=c("blue", "red"), lwd=3)

lm.fit16 <- lm(formula = y ~ I(x^2) + x)
summary(lm.fit16)
# There is no evidence that a quadratic term helps fit
# The R^2 term increased slightly but the squared x is not statistically significant 

# Create less noise in the model
set.seed(1)
eps2 <- rnorm(100, 0, 0.125)
x2 <- rnorm(100)
y2 <- -1 + 0.5*x2 + eps2
plot(x2, y2)
lm.fit17 <- lm(formula = y2 ~ x2)
abline(lm.fit17, lwd = 3, col = "red")
summary(lm.fit17)

abline(-1, 0.5, lwd = 3, col = "blue")
legend(-2, legend = c("population", "model"), col = c("blue", "red"), lwd = 3)

# Create more noise in the model
set.seed(1)
eps3 <- rnorm(100, 0, 0.5)
x3 <- rnorm(100)
y3 <- -1 + 0.5*x3 + eps3
plot(x3,y3)
lm.fit18 <- lm(formula = y3 ~ x3)
abline(lm.fit18, lwd = 3, col = "red")
summary(lm.fit18)

abline(-1, 0.5, lwd = 3, col = "blue")
legend(-2, legend = c("population", "model"), col = c("blue", "red"), lwd = 3)

# Create confidence intervals
confint(lm.fit15)
confint(lm.fit17)
confint(lm.fit18)
# Naturally, less noise means a tighter confidence interval vs more noise

#####################################################
# HW 3 Question 14
#####################################################

set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)

# Find correlation between variables
cor(x1, x2)
plot(x1, x2)

# Plot fitted line on scatterplot
lm.fit19 <- lm(formula = y ~ x1 + x2)
summary(lm.fit19)
# The estimated coefficient for x2 is not stastically significant, and this is proven by the fact
# the estimated coefficient value is far off from the true population coefficient.

lm.fit20 <- lm(formula = y ~ x1)
summary(lm.fit20)

lm.fit21 <- lm(formula = y ~ x2)
summary(lm.fit21)

# These two linear models contradict the multiple regression model
# This is because of collinearity - meaning that it is difficult for the model
# to distinguish individual effects of the variables 
# Therefore, the variables have more variance which in term affects their t-values
# Which makes us incorrectly assert the null hypothesis

x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y,6)

lm.fit22 <- lm(formula = y ~ x1)
par(mfrow = c(2,2))
plot(lm.fit22)
summary(lm.fit22)

lm.fit23 <- lm(formula = y ~ x2)
par(mfrow = c(2,2))
plot(lm.fit23)
summary(lm.fit23)

lm.fit24 <- lm(formula = y ~ x1 + x2)
par(mfrow = c(2,2))
plot(lm.fit24)
summary(lm.fit24)

#####################################################
# HW 3 Question 15
#####################################################

library(MASS)
Boston <- data.frame(Boston)
str(Boston)

# Predict per capital crime against individual predictors

lm.zn <- lm(formula = crim ~ zn, Boston)
lm.indus <- lm(crim~indus, Boston)
lm.chas <- lm(crim~chas, Boston)
lm.nox <- lm(crim~nox, Boston)
lm.rm <- lm(crim~rm, Boston)
lm.age <- lm(crim~age, Boston)
lm.dis <- lm(crim~dis, Boston)
lm.rad <- lm(crim~rad, Boston)
lm.tax <- lm(crim~tax, Boston)
lm.ptratio <- lm(crim~ptratio, Boston)
lm.black <- lm(crim~black, Boston)
lm.stat <- lm(crim~lstat, Boston)
lm.medv <- lm(crim~medv, Boston)

# Predict per capital crime
crime.rate <- lm(formula = crim ~ ., Boston)
summary(crime.rate)

# Scatterplot of individual coefficients vs multiple regression coefficients
x <- c(coefficients(lm.zn)[2],
      coefficients(lm.indus)[2],
      coefficients(lm.chas)[2],
      coefficients(lm.nox)[2],
      coefficients(lm.rm)[2],
      coefficients(lm.age)[2],
      coefficients(lm.dis)[2],
      coefficients(lm.rad)[2],
      coefficients(lm.tax)[2],
      coefficients(lm.ptratio)[2],
      coefficients(lm.black)[2],
      coefficients(lm.stat)[2],
      coefficients(lm.medv)[2])
y <- coefficients(crime.rate)[2:14]
plot(x, y)

# Try polynomial regressions
lm.zn <- lm(formula = crim ~ poly(zn,3), Boston)
lm.indus <- lm(crim~poly(indus,3), Boston)
# lm.chas <- lm(crim~poly(chas,3), Boston) ; doesn't work as it's binary
lm.nox <- lm(crim~poly(nox,3), Boston)
lm.rm <- lm(crim~poly(rm,3), Boston)
lm.age <- lm(crim~poly(age,3), Boston)
lm.dis <- lm(crim~poly(dis,3), Boston)
lm.rad <- lm(crim~poly(rad,3), Boston)
lm.tax <- lm(crim~poly(tax,3), Boston)
lm.ptratio <- lm(crim~poly(ptratio,3), Boston)
lm.black <- lm(crim~poly(black,3), Boston)
lm.stat <- lm(crim~poly(lstat,3), Boston)
lm.medv <- lm(crim~poly(medv,3), Boston)